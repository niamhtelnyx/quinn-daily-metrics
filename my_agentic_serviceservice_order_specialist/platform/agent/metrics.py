"""Prometheus metrics for agent execution.

This module provides plug-and-play metrics collection for agents using
context managers and decorators. Designed for multi-agent services.

Usage:
    # Agent-level metrics (context manager)
    async with collect_agent_metrics(AgentMetricsLabels(agent="my-agent")):
        result = await agent.run(...)

    # Tool-level metrics (function call)
    record_tool_call(ToolMetricsLabels("my-agent", "tool-name"), duration=1.5, error=False)
"""

from collections.abc import AsyncIterator
from contextlib import asynccontextmanager
from time import monotonic
from types import TracebackType
from typing import NamedTuple

import prometheus_client

from my_agentic_serviceservice_order_specialist.platform.constants import SERVICE_NAME, SQUAD_NAME
from my_agentic_serviceservice_order_specialist.platform.observability.metrics import BUCKETS


class AgentMetricsLabels(NamedTuple):
    """Labels for agent-level metrics."""

    agent: str


class ToolMetricsLabels(NamedTuple):
    """Labels for tool-level metrics."""

    agent: str
    tool_name: str
    proxy_tool_name: str = ""


# --- Agent Metrics ---

agent_run_total = prometheus_client.Counter(
    name="agent_run_total",
    documentation="Total number of agent runs",
    labelnames=["agent", "squad", "service", "status"],
    registry=prometheus_client.REGISTRY,
)

agent_run_duration_seconds = prometheus_client.Histogram(
    name="agent_run_duration_seconds",
    documentation="Duration of complete agent runs in seconds",
    labelnames=["agent", "squad", "service", "status"],
    buckets=BUCKETS,
    registry=prometheus_client.REGISTRY,
)

agent_input_tokens_total = prometheus_client.Counter(
    name="agent_input_tokens_total",
    documentation="Total input tokens consumed by agents",
    labelnames=["agent", "squad", "service", "model"],
    registry=prometheus_client.REGISTRY,
)

agent_output_tokens_total = prometheus_client.Counter(
    name="agent_output_tokens_total",
    documentation="Total output tokens generated by agents",
    labelnames=["agent", "squad", "service", "model"],
    registry=prometheus_client.REGISTRY,
)

# --- Tool Metrics ---

agent_tool_call_total = prometheus_client.Counter(
    name="agent_tool_call_total",
    documentation="Total number of tool calls by agents",
    labelnames=["agent", "squad", "service", "tool_name", "proxy_tool_name", "status"],
    registry=prometheus_client.REGISTRY,
)

agent_tool_call_duration_seconds = prometheus_client.Histogram(
    name="agent_tool_call_duration_seconds",
    documentation="Duration of tool calls in seconds",
    labelnames=["agent", "squad", "service", "tool_name", "proxy_tool_name", "status"],
    buckets=BUCKETS,
    registry=prometheus_client.REGISTRY,
)


# --- Context Managers ---


class collect_agent_metrics:  # noqa: N801
    """Async context manager/decorator to collect agent run metrics.

    Records run count, duration, and success/error status.

    Usage as context manager:
        async with collect_agent_metrics(AgentMetricsLabels("my-agent")):
            result = await agent.run(message)

    Usage as decorator:
        @collect_agent_metrics(AgentMetricsLabels("my-agent"))
        async def run_agent():
            ...
    """

    def __init__(self, labels: AgentMetricsLabels):
        self.labels = labels
        self.start = monotonic()

    async def __aenter__(self) -> "collect_agent_metrics":
        self.start = monotonic()
        return self

    async def __aexit__(
        self,
        exc_type: type[BaseException] | None,
        exc: BaseException | None,
        tb: TracebackType | None,
    ) -> bool:
        elapsed = monotonic() - self.start
        status = "error" if exc else "success"

        agent_run_total.labels(self.labels.agent, SQUAD_NAME, SERVICE_NAME, status).inc()
        agent_run_duration_seconds.labels(
            self.labels.agent, SQUAD_NAME, SERVICE_NAME, status
        ).observe(elapsed)

        return False  # Don't suppress exceptions


@asynccontextmanager
async def collect_tool_metrics(labels: ToolMetricsLabels) -> AsyncIterator[None]:
    """Async context manager to collect tool call metrics.

    Usage:
        async with collect_tool_metrics(ToolMetricsLabels("my-agent", "my-tool")):
            result = await tool.execute(...)
    """
    start = monotonic()
    status = "success"
    try:
        yield
    except Exception:
        status = "error"
        raise
    finally:
        elapsed = monotonic() - start
        agent_tool_call_total.labels(
            labels.agent,
            SQUAD_NAME,
            SERVICE_NAME,
            labels.tool_name,
            labels.proxy_tool_name,
            status,
        ).inc()
        agent_tool_call_duration_seconds.labels(
            labels.agent,
            SQUAD_NAME,
            SERVICE_NAME,
            labels.tool_name,
            labels.proxy_tool_name,
            status,
        ).observe(elapsed)


# --- Helper Functions ---


def record_tool_call(
    labels: ToolMetricsLabels,
    duration: float,
    error: bool = False,
) -> None:
    """Record a tool call metric (for cases where exceptions are caught).

    Use this when tool errors are caught and converted to error responses
    rather than raised as exceptions.

    Args:
        labels: Tool metric labels (agent, tool_name)
        duration: Duration in seconds
        error: Whether the tool call resulted in an error
    """
    status = "error" if error else "success"
    agent_tool_call_total.labels(
        labels.agent,
        SQUAD_NAME,
        SERVICE_NAME,
        labels.tool_name,
        labels.proxy_tool_name,
        status,
    ).inc()
    agent_tool_call_duration_seconds.labels(
        labels.agent,
        SQUAD_NAME,
        SERVICE_NAME,
        labels.tool_name,
        labels.proxy_tool_name,
        status,
    ).observe(duration)


def record_agent_tokens(
    agent: str,
    model: str,
    input_tokens: int,
    output_tokens: int,
) -> None:
    """Record token usage for an agent run.

    Args:
        agent: The agent's identifier
        model: The LLM model name
        input_tokens: Number of input tokens consumed
        output_tokens: Number of output tokens generated
    """
    if input_tokens > 0:
        agent_input_tokens_total.labels(
            agent=agent, squad=SQUAD_NAME, service=SERVICE_NAME, model=model
        ).inc(input_tokens)
    if output_tokens > 0:
        agent_output_tokens_total.labels(
            agent=agent, squad=SQUAD_NAME, service=SERVICE_NAME, model=model
        ).inc(output_tokens)
